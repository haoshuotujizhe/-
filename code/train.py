import torch
import torch.nn as nn
import os
import math
import random
import json
import torch.optim as optim
from tqdm import tqdm
import numpy as np
import copy
import sys
# å…¼å®¹æ–°æ—§ AMP API
try:
    from torch.amp import autocast, GradScaler
    AMP_HAS_DEVICE = True
except ImportError:
    from torch.cuda.amp import autocast, GradScaler
    AMP_HAS_DEVICE = False


sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from model import build_model
from utils import get_dataloaders, save_model, set_seed

def rand_bbox(W, H, lam):
    cut_rat = math.sqrt(1. - lam)
    cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)
    cx, cy = random.randint(0, W), random.randint(0, H)
    x1, y1 = np.clip(cx - cut_w // 2, 0, W), np.clip(cy - cut_h // 2, 0, H)
    x2, y2 = np.clip(cx + cut_w // 2, 0, W), np.clip(cy + cut_h // 2, 0, H)
    return x1, y1, x2, y2

def apply_mixup_cutmix(images, targets, use_mixup, mixup_alpha, use_cutmix, cutmix_alpha):
    """è¿”å› (images, (y_a, y_b, lam)) æˆ– (images, (targets, None, 1.0))"""
    if use_cutmix and cutmix_alpha > 0 and random.random() < 0.5:
        lam = np.random.beta(cutmix_alpha, cutmix_alpha)
        batch_size, _, H, W = images.size()
        index = torch.randperm(batch_size, device=images.device)
        x1, y1, x2, y2 = rand_bbox(W, H, lam)
        images[:, :, y1:y2, x1:x2] = images[index, :, y1:y2, x1:x2]
        lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))
        return images, (targets, targets[index], lam)
    if use_mixup and mixup_alpha > 0:
        lam = np.random.beta(mixup_alpha, mixup_alpha)
        batch_size = images.size(0)
        index = torch.randperm(batch_size, device=images.device)
        mixed = lam * images + (1 - lam) * images[index, :]
        return mixed, (targets, targets[index], lam)
    return images, (targets, None, 1.0)

class ModelEMA:
    def __init__(self, model, decay=0.9998, device=None):
        self.ema = copy_model(model)
        self.ema.eval()
        for p in self.ema.parameters():
            p.requires_grad_(False)
        self.decay = decay
        self.device = device
        if device is not None:
            self.ema = self.ema.to(device)

    @torch.no_grad()
    def update(self, model):
        # âœ… åŒæ­¥å‚æ•° + BN buffersï¼ˆé¿å… EMA éªŒè¯æ—¶ç»Ÿè®¡é‡è¿‡æœŸï¼‰
        d = self.decay
        msd = model.state_dict()
        esd = self.ema.state_dict()
        for k in esd.keys():
            if not torch.is_floating_point(esd[k]):
                # int bufferï¼ˆnum_batches_tracked ç­‰ï¼‰ç›´æ¥è¦†ç›–
                esd[k].copy_(msd[k])
            else:
                if ("running_mean" in k) or ("running_var" in k):
                    # BN running stats ç›´æ¥åŒæ­¥ï¼Œä¸åšæ»‘åŠ¨å¹³å‡
                    esd[k].copy_(msd[k])
                else:
                    # å…¶ä½™å¯è®­ç»ƒæµ®ç‚¹å‚æ•°åš EMA
                    esd[k].mul_(d).add_(msd[k], alpha=1.0 - d)

def copy_model(model):
    import copy
    m = copy.deepcopy(model)
    return m

def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler=None, use_amp=False, max_grad_norm=1.0, 
                    aug_cfg=None, ema=None, amp_dtype=torch.float16):
    model.train()
    total_loss, total_correct, total = 0, 0, 0
    for imgs, labels in tqdm(dataloader, desc="Training", leave=False):
        imgs, labels = imgs.to(device,non_blocking=True), labels.to(device,non_blocking=True)
        original_labels = labels.clone()
        imgs, (y_a, y_b, lam) = apply_mixup_cutmix(
            imgs, labels,
            use_mixup=aug_cfg.get("use_mixup", False)if aug_cfg else False,
            mixup_alpha=aug_cfg.get("mixup_alpha", 0.0)if aug_cfg else 0.0,
            use_cutmix=aug_cfg.get("use_cutmix", False)if aug_cfg else False,
            cutmix_alpha=aug_cfg.get("cutmix_alpha", 0.0)if aug_cfg else 0.0
        )
        optimizer.zero_grad(set_to_none=True)
        if AMP_HAS_DEVICE:
            with autocast(device_type="cuda",dtype=amp_dtype,enabled=use_amp):
                outputs = model(imgs)
                if y_b is None:
                    loss = criterion(outputs, y_a)
                else:
                    loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)
        else:
            with autocast(enabled=use_amp):
                outputs = model(imgs)
                if y_b is None:
                    loss = criterion(outputs, y_a)
                else:
                    loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)

        if use_amp and scaler is not None:
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()

        if ema is not None:
            ema.update(model)

        total_loss += loss.item() * imgs.size(0)
        _, preds = torch.max(outputs, 1)
        # æ³¨æ„ï¼šä½¿ç”¨ Mixup/CutMix æ—¶ï¼Œæ­¤å‡†ç¡®ç‡ä¸ºè¿‘ä¼¼å€¼ï¼ˆå®é™…æ ‡ç­¾å·²æ··åˆï¼‰ï¼Œä»…ä¾›å‚è€ƒ
        total_correct += (preds == original_labels).sum().item()  
        total += labels.size(0)
    return total_loss / total, total_correct / total

def validate(model, dataloader, criterion, device,use_amp=False, use_tta=False, amp_dtype=torch.float16):
    model.eval()
    total_loss, total_correct, total = 0, 0, 0
    with torch.no_grad():
        for imgs, labels in tqdm(dataloader, desc="Validating", leave=False):
            imgs, labels = imgs.to(device), labels.to(device)
            # ç®€å•TTAï¼šåŸå›¾ + æ°´å¹³ç¿»è½¬
            if use_tta:
                imgs_flipped = torch.flip(imgs, dims=[3])
                if AMP_HAS_DEVICE:
                    with autocast(device_type="cuda",dtype=amp_dtype,enabled=use_amp):
                        out1 = model(imgs)
                        out2 = model(imgs_flipped)
                        outputs = 0.5 * (out1 + out2)
                        loss = criterion(outputs, labels)
                else:
                    with autocast(enabled=use_amp):
                        out1 = model(imgs)
                        out2 = model(imgs_flipped)
                        outputs = 0.5 * (out1 + out2)
                        loss = criterion(outputs, labels)
            else:
                if AMP_HAS_DEVICE:
                    with autocast(device_type="cuda",dtype=amp_dtype,enabled=use_amp):
                        outputs = model(imgs)
                        loss = criterion(outputs, labels)
                else:
                    with autocast(enabled=use_amp):
                        outputs = model(imgs)
                        loss = criterion(outputs, labels)
                    
            total_loss += loss.item() * imgs.size(0)
            _, preds = torch.max(outputs, 1)
            total_correct += (preds == labels).sum().item()
            total += labels.size(0)
    return total_loss / total, total_correct / total

def set_trainable(module, flag: bool):
    for p in module.parameters():
        p.requires_grad_(flag)

def build_optimizer(model, lr_backbone, lr_head, weight_decay=5e-4):  # âœ… ä» 1e-4 æå‡åˆ° 5e-4
    """åˆ¤åˆ«å¼å­¦ä¹ ç‡ + æ­£ç¡®çš„æƒé‡è¡°å‡æ’é™¤ï¼ˆbn/biasä¸è¡°å‡ï¼‰"""
    decay, no_decay = [], []
    for name, p in model.named_parameters():
        if not p.requires_grad:
            continue
        if p.ndim == 1 or name.endswith(".bias"):
            no_decay.append(p)
        else:
            decay.append(p)
    # åˆ†ç±»å¤´å‚æ•°å•ç‹¬åˆ†ç»„ï¼ˆæ›´å¤§å­¦ä¹ ç‡ï¼‰
    head_params = list(model.backbone.classifier.parameters())
    head_id = set([id(p) for p in head_params])

    decay_backbone = [p for p in decay if id(p) not in head_id]
    no_decay_backbone = [p for p in no_decay if id(p) not in head_id]
    decay_head = [p for p in decay if id(p) in head_id]
    no_decay_head = [p for p in no_decay if id(p) in head_id]

    param_groups = [
        {"params": decay_backbone, "lr": lr_backbone, "weight_decay": weight_decay},
        {"params": no_decay_backbone, "lr": lr_backbone, "weight_decay": 0.0},
        {"params": decay_head, "lr": lr_head, "weight_decay": weight_decay},
        {"params": no_decay_head, "lr": lr_head, "weight_decay": 0.0},
    ]
    return optim.AdamW(param_groups)


if __name__ == "__main__":
    code_dir = os.path.dirname(os.path.abspath(__file__))  # ä¾‹å¦‚ï¼š/xxx/xxx/code
# å›åˆ° code çš„ä¸Šä¸€çº§ç›®å½•ï¼ˆå³ code å’Œ model çš„å…±åŒçˆ¶ç›®å½•ï¼‰
    parent_dir = os.path.dirname(code_dir)  # ä¾‹å¦‚ï¼š/xxx/xxx
# æ‹¼æ¥å‡º config.json çš„ç»å¯¹è·¯å¾„ï¼ˆçˆ¶ç›®å½• -> model -> config.jsonï¼‰
    config_path = os.path.join(parent_dir, "model", "config.json")  # ä¾‹å¦‚ï¼š/xxx/xxx/model/config.json
    with open(config_path, "r") as f:
        config = json.load(f)
        
    #å°†é…ç½®ä¸­çš„ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    def resolve_path(path):
        if os.path.isabs(path):
            return path
        return os.path.abspath(os.path.join(parent_dir, path))
    
    config["train_dir"] = resolve_path(config["train_dir"])
    config["val_dir"] = resolve_path(config["val_dir"])
    config["test_dir"] = resolve_path(config["test_dir"])
    config["train_label_csv"] = resolve_path(config["train_label_csv"])
    
    print("Resolved paths:")
    print(f"  train_dir: {config['train_dir']}")
    print(f"  val_dir: {config['val_dir']}")
    print(f"  train_label_csv: {config['train_label_csv']}")

    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_amp = device.type == "cuda"
    # æ ¹æ®è®¾å¤‡é€‰æ‹© dtypeï¼ˆAmpereèµ·ä¼˜å…ˆç”¨ bf16ï¼‰
    if device.type == "cuda" and hasattr(torch.cuda, "is_bf16_supported") and torch.cuda.is_bf16_supported():
        amp_dtype = torch.bfloat16
    else:
        amp_dtype = torch.float16
    amp_device = "cuda" if device.type == "cuda" else "cpu"

    #scaler = GradScaler('cuda',enabled=use_amp)
    if AMP_HAS_DEVICE:
        scaler = GradScaler(enabled=use_amp, device='cuda' if device.type == 'cuda' else 'cpu')
    else:
        scaler = GradScaler(enabled=use_amp)
    
    # æ•°æ®åŠ è½½
    train_loader, val_loader = get_dataloaders(
        train_dir=config["train_dir"],
        train_label_csv=config["train_label_csv"],
        val_dir=config["val_dir"],
        config=config
    )

    # è¯»å–ç­–ç•¥é…ç½®
    head_epochs = int(config.get("head_epochs", 0))
    lr_backbone = float(config.get("lr_backbone", config["learning_rate"]))
    lr_head = float(config.get("lr_head", config["learning_rate"]))
    use_tta = bool(config.get("use_tta", False))
    
    aug_cfg = {
        "use_mixup": bool(config.get("use_mixup", False)),
        "mixup_alpha": float(config.get("mixup_alpha", 0.0)),
        "use_cutmix": bool(config.get("use_cutmix", False)),
        "cutmix_alpha": float(config.get("cutmix_alpha", 0.0)),
    }
    
    # æ¨¡å‹
    model = build_model(config).to(device)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.02)  # âœ… ä» 0.05 é™åˆ° 0.02
    
    # âœ… Phase 1 ä¸åˆå§‹åŒ– EMA
    ema = None
    
    # å®šä¹‰å¢å¼ºé…ç½®
    aug_cfg_phase1 = {
        "use_mixup": False,
        "mixup_alpha": 0.0,
        "use_cutmix": False,
        "cutmix_alpha": 0.0,
    }
    
    aug_cfg_phase2 = {
        "use_mixup": bool(config.get("use_mixup", False)),
        "mixup_alpha": float(config.get("mixup_alpha", 0.0)),
        "use_cutmix": bool(config.get("use_cutmix", False)),
        "cutmix_alpha": float(config.get("cutmix_alpha", 0.0)),
    }
    
    print(f"âœ… å¢å¼ºé…ç½®å·²è®¾ç½®:")
    print(f"   Phase 1 - Mixup: {aug_cfg_phase1['use_mixup']}, CutMix: {aug_cfg_phase1['use_cutmix']}")
    print(f"   Phase 2 - Mixup: {aug_cfg_phase2['use_mixup']}, CutMix: {aug_cfg_phase2['use_cutmix']}")
    print(f"   Label Smoothing: 0.02")  # âœ… æ›´æ–°æ˜¾ç¤º

    # é˜¶æ®µ1ï¼šåªè®­ç»ƒåˆ†ç±»å¤´
    if head_epochs > 0:
        print(f"\n=== Phase 1: Train classifier only for {head_epochs} epochs ===")
        print("âš ï¸  Phase 1 ä¸ä½¿ç”¨ EMAï¼ˆåˆ†ç±»å¤´ä»é›¶å¼€å§‹è®­ç»ƒï¼‰")
        
        set_trainable(model.backbone.features, False)
        set_trainable(model.backbone.classifier, True)
        optimizer = build_optimizer(model, lr_backbone=0.0, lr_head=lr_head)
        
        best_acc = 0.0
        model_save_dir = os.path.join(parent_dir, "model")
        os.makedirs(model_save_dir, exist_ok=True)
        phase1_model_path = os.path.join(model_save_dir, "phase1_best.pth")
        
        for epoch in range(head_epochs):
            print(f"\n[Phase1] Epoch {epoch+1}/{head_epochs}")
            # âœ… Phase 1 ä¸ä½¿ç”¨ EMAï¼ˆä¼ å…¥ ema=Noneï¼‰
            train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, 
                                                   scaler, use_amp, aug_cfg=aug_cfg_phase1, ema=None, amp_dtype=amp_dtype)
            # âœ… Phase 1 ç›´æ¥éªŒè¯ä¸»æ¨¡å‹ï¼ˆä¸ç”¨ EMAï¼‰
            val_loss, val_acc = validate(model, val_loader, criterion, device, use_amp, 
                                        use_tta=False, amp_dtype=amp_dtype)
            print(f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}")
            
            if val_acc > best_acc:
                best_acc = val_acc
                save_model(model, phase1_model_path)
                print("âœ… Saved new best model (Phase1)")
                
        # Phase 2 ä» Phase 1 æœ€ä½³æƒé‡å¼€å§‹
        print(f"\nLoading Phase1 best model (Val Acc: {best_acc:.4f})")
        model.load_state_dict(torch.load(phase1_model_path, weights_only=True))

    # âœ… Phase 2 å¼€å§‹æ—¶æ‰åˆå§‹åŒ– EMA
    print("\n=== Phase 2: Fine-tune full network ===")
    print("ğŸ”„ é‡æ–°åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨å¼ºå¢å¼ºï¼‰...")
    
    # âœ… é‡æ–°åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨å¼ºå¢å¼ºï¼‰
    train_loader, _ = get_dataloaders(
        train_dir=config["train_dir"],
        train_label_csv=config["train_label_csv"],
        val_dir=config["val_dir"],
        config=config,
        use_strong_aug=True  # âœ… Phase 2 ä½¿ç”¨å¼ºå¢å¼º
    )
    
    if config.get("use_ema", False):
        print("âœ… åˆå§‹åŒ– EMAï¼ˆPhase 2ï¼‰")
        ema = ModelEMA(model, decay=float(config.get("ema_decay", 0.9998)), device=device)
    else:
        ema = None
    
    set_trainable(model, True)
    
    # âœ… ä½¿ç”¨ Warmup + CosineAnnealingWarmRestartsï¼ˆæ›´ç¨³å®šï¼Œå‘¨æœŸæ€§é‡å¯ï¼‰
    optimizer = build_optimizer(model, lr_backbone=lr_backbone, lr_head=lr_head, weight_decay=5e-4)  # âœ… å¢åŠ æƒé‡è¡°å‡
    from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR
    
    # Warmup 3 ä¸ª epoch
    def warmup_lambda(epoch):
        return (epoch + 1) / 3 if epoch < 3 else 1.0

    warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)
    # âœ… ä¸é‡å¯çš„ä½™å¼¦é€€ç«ï¼Œæ•´ä¸ª Phase2 å¹³æ»‘ä¸‹é™
    cosine_scheduler = CosineAnnealingLR(optimizer, T_max=int(config["epochs"]), eta_min=1e-6)

    best_acc = 0.0
    patience_counter = 0
    max_patience = 10       # æ”¾å®½æ—©åœï¼ˆå› ä¸ºå‘¨æœŸæ€§é‡å¯ï¼‰
    min_epochs = 25         # æœ€å°è®­ç»ƒè½®æ•°
    min_delta = 1e-4
    best_model_path = os.path.join(model_save_dir, "best_model.pth")

    for epoch in range(config["epochs"]):
        # âœ… å‰ 3 ä¸ª epoch ä½¿ç”¨ warmupï¼Œä¹‹åä½¿ç”¨ cosine
        if epoch < 3:
            current_lr = optimizer.param_groups[0]['lr'] * warmup_lambda(epoch)
        else:
            current_lr = optimizer.param_groups[0]['lr']
        
        print(f"\n[Phase2] Epoch {epoch+1}/{config['epochs']} LR: {current_lr:.6f}")
        
        # âœ… åŠ¨æ€å‡å¼±å¢å¼ºï¼ˆç¬¬ 15 ä¸ª epoch åï¼‰
        cur_aug = dict(aug_cfg_phase2)
        if epoch >= 12:
            cur_aug.update({"use_cutmix": False, "mixup_alpha": 0.05})
            if epoch == 12:
                print("   ğŸ“‰ å‡å¼±å¢å¼ºï¼šå·²å…³é—­ CutMixï¼ŒMixup alpha=0.05")
        if epoch >= 18:
            cur_aug.update({"use_mixup": False, "mixup_alpha": 0.0, "use_cutmix": False})
            if epoch == 18:
                print("   ğŸ“´ å½»åº•å…³é—­ Mixup/CutMix ä»¥æ”¶æ•›")
        
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device,
            scaler, use_amp, aug_cfg=cur_aug, ema=ema, amp_dtype=amp_dtype
        )
        eval_model = ema.ema if ema is not None else model
        if ema is not None:
            # âœ… å†æ¬¡ç¡®ä¿ BN buffers ä¸å½“å‰æ¨¡å‹ä¸€è‡´
            for (n, b_ema) in eval_model.named_buffers():
                b_model = dict(model.named_buffers())[n]
                b_ema.copy_(b_model)
        val_loss, val_acc = validate(eval_model, val_loader, criterion, device, use_amp, 
                                    use_tta=False, amp_dtype=amp_dtype)
        print(f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}")

        # âœ… å­¦ä¹ ç‡è°ƒåº¦
        if epoch < 3:
            warmup_scheduler.step()
        else:
            cosine_scheduler.step()

        if val_acc > best_acc + min_delta:
            best_acc = val_acc
            patience_counter = 0
            save_model(eval_model, best_model_path)
            print("âœ… Saved new best model!")
        else:
            patience_counter += 1
            print(f"âš ï¸ No improvement for {patience_counter} epochs")

        # âœ… è‡³å°‘è®­ç»ƒ min_epochs åæ‰æ—©åœ
        if (epoch + 1) >= min_epochs and patience_counter >= max_patience:
            print("Early stopping triggered")
            break

    print(f"\nTraining complete! Best Val Accuracy: {best_acc:.4f}")

    # âœ… Phase 3: é«˜åˆ†è¾¨ç‡å¾®è°ƒï¼ˆå¯æ˜¾è‘—æŠ¬æœ€å 0.3~0.8%ï¼‰
    if bool(config.get("final_finetune", True)):
        print("\n=== Phase 3: High-res fine-tune ===")
        cfg_p3 = dict(config)
        cfg_p3["input_size"] = list(config.get("final_input_size", [600, 600]))
        # å…³å¼ºå¢å¼ºï¼Œåªä¿ç•™è½»å¢å¼ºæˆ– CenterCropï¼ˆåœ¨ utils é‡ŒæŒ‰ use_strong_aug=Falseï¼‰
        train_loader_p3, _ = get_dataloaders(
            train_dir=cfg_p3["train_dir"],
            train_label_csv=cfg_p3["train_label_csv"],
            val_dir=cfg_p3["val_dir"],
            config=cfg_p3,
            use_strong_aug=False
        )

        # å°å­¦ä¹ ç‡å¾®è°ƒï¼ˆå…¨éƒ¨å‚æ•°ï¼‰
        lr_mult = float(config.get("final_lr_mult", 0.2))
        for pg in optimizer.param_groups:
            pg["lr"] = max(pg["lr"] * lr_mult, 1e-6)

        # å…³é—­ Mixup/CutMix
        aug_p3 = {"use_mixup": False, "mixup_alpha": 0.0, "use_cutmix": False, "cutmix_alpha": 0.0}

        final_epochs = int(config.get("final_epochs", 3))
        for e in range(final_epochs):
            print(f"\n[Phase3] Epoch {e+1}/{final_epochs}")
            trl, tra = train_one_epoch(model, train_loader_p3, criterion, optimizer, device,
                                       scaler, use_amp, aug_cfg=aug_p3, ema=ema, amp_dtype=amp_dtype)
            eval_model = ema.ema if ema is not None else model
            if ema is not None:
                for (n, b_ema) in eval_model.named_buffers():
                    b_model = dict(model.named_buffers())[n]
                    b_ema.copy_(b_model)
            vll, vla = validate(eval_model, val_loader, criterion, device, use_amp, use_tta=True, amp_dtype=amp_dtype)
            print(f"Train Loss: {trl:.4f}, Acc: {tra:.4f} | Val Loss: {vll:.4f}, Acc: {vla:.4f}")
            if vla > best_acc:
                best_acc = vla
                save_model(eval_model, best_model_path)
                print("âœ… Saved new best model (Phase3)")