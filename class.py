import os
import random
import shutil
import json
import numpy as np
import pandas as pd
from collections import Counter


def analyze_distribution(df):
    """åˆ†æç±»åˆ«åˆ†å¸ƒ"""
    category_counts = df['category_id'].value_counts()
    
    print(f"\n{'='*60}")
    print("ğŸ“Š ç±»åˆ«åˆ†å¸ƒåˆ†æ")
    print(f"{'='*60}")
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"ç±»åˆ«æ•°: {len(category_counts)}")
    print(f"æ ·æœ¬æ•°èŒƒå›´: {category_counts.min()} ~ {category_counts.max()}")
    print(f"å¹³å‡æ ·æœ¬æ•°: {category_counts.mean():.1f}")
    print(f"ä¸­ä½æ•°: {category_counts.median():.1f}")
    
    # é•¿å°¾ç»Ÿè®¡
    bins = [0, 20, 50, 100, 200, float('inf')]
    labels = ['â‰¤20', '21-50', '51-100', '101-200', '>200']
    for i in range(len(bins) - 1):
        count = ((category_counts >= bins[i]) & (category_counts < bins[i+1])).sum()
        print(f"   {labels[i]} æ ·æœ¬: {count} ä¸ªç±»åˆ«")
    
    return category_counts


def stratified_split(df, val_ratio=0.15, min_val_per_class=2, seed=42):
    """
    åˆ†å±‚åˆ’åˆ†æ•°æ®é›†
    - æ¯ä¸ªç±»åˆ«æŒ‰æ¯”ä¾‹åˆ’åˆ†
    - å°æ ·æœ¬ç±»åˆ«ä¿è¯è‡³å°‘æœ‰éªŒè¯æ ·æœ¬
    """
    random.seed(seed)
    np.random.seed(seed)
    
    train_indices = []
    val_indices = []
    
    grouped = df.groupby('category_id')
    
    for cat_id, group in grouped:
        indices = group.index.tolist()
        n = len(indices)
        random.shuffle(indices)
        
        # éªŒè¯é›†æ•°é‡ï¼ˆè‡³å°‘ min_val_per_class ä¸ªï¼‰
        n_val = max(min_val_per_class, int(n * val_ratio))
        
        # ç¡®ä¿è®­ç»ƒé›†è‡³å°‘æœ‰ 2 ä¸ªæ ·æœ¬
        if n - n_val < 2:
            n_val = max(1, n - 2)
        
        val_indices.extend(indices[:n_val])
        train_indices.extend(indices[n_val:])
    
    train_df = df.loc[train_indices].reset_index(drop=True)
    val_df = df.loc[val_indices].reset_index(drop=True)
    
    return train_df, val_df


def dataset_classified(config):
    """å°†åŸå§‹æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    
    original_dir = config["original_train_dir"]
    train_dir = config["train_dir"]
    val_dir = config["val_dir"]
    csv_path = config["train_label_csv"]
    val_ratio = config.get("val_ratio", 0.15)
    
    print(f"\n{'='*60}")
    print("ğŸš€ å¼€å§‹æ•°æ®é›†åˆ’åˆ†")
    print(f"{'='*60}")
    print(f"åŸå§‹ç›®å½•: {original_dir}")
    print(f"è®­ç»ƒé›†ç›®å½•: {train_dir}")
    print(f"éªŒè¯é›†ç›®å½•: {val_dir}")
    print(f"éªŒè¯é›†æ¯”ä¾‹: {val_ratio}")
    
    # è¯»å– CSV
    df = pd.read_csv(csv_path)
    print(f"\nCSV æ€»è®°å½•æ•°: {len(df)}")
    
    # æ£€æŸ¥åŸå§‹ç›®å½•
    if not os.path.exists(original_dir):
        print(f"âŒ åŸå§‹ç›®å½•ä¸å­˜åœ¨: {original_dir}")
        return None, None
    
    existing_files = set(os.listdir(original_dir))
    print(f"åŸå§‹ç›®å½•æ–‡ä»¶æ•°: {len(existing_files)}")
    
    # è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
    df = df[df['filename'].isin(existing_files)].reset_index(drop=True)
    print(f"åŒ¹é…åˆ°çš„è®°å½•æ•°: {len(df)}")
    
    if len(df) == 0:
        print("âŒ æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ–‡ä»¶ï¼")
        return None, None
    
    # åˆ†æåˆ†å¸ƒ
    analyze_distribution(df)
    
    # åˆ†å±‚åˆ’åˆ†
    train_df, val_df = stratified_split(df, val_ratio=val_ratio, min_val_per_class=2, seed=42)
    
    print(f"\n{'='*60}")
    print("ğŸ“‚ åˆ’åˆ†ç»“æœ")
    print(f"{'='*60}")
    print(f"è®­ç»ƒé›†: {len(train_df)} å¼  ({len(train_df)/len(df)*100:.1f}%)")
    print(f"éªŒè¯é›†: {len(val_df)} å¼  ({len(val_df)/len(df)*100:.1f}%)")
    
    # æ£€æŸ¥ç±»åˆ«è¦†ç›–
    train_cats = set(train_df['category_id'].unique())
    val_cats = set(val_df['category_id'].unique())
    
    print(f"\nè®­ç»ƒé›†ç±»åˆ«æ•°: {len(train_cats)}")
    print(f"éªŒè¯é›†ç±»åˆ«æ•°: {len(val_cats)}")
    
    missing = train_cats - val_cats
    if missing:
        print(f"âš ï¸ éªŒè¯é›†ç¼ºå°‘ {len(missing)} ä¸ªç±»åˆ«")
    else:
        print("âœ… éªŒè¯é›†åŒ…å«æ‰€æœ‰è®­ç»ƒé›†ç±»åˆ«")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(val_dir, exist_ok=True)
    
    # ç§»åŠ¨æ–‡ä»¶
    print(f"\nğŸ“¦ ç§»åŠ¨æ–‡ä»¶ä¸­...")
    
    moved_train = 0
    moved_val = 0
    
    for _, row in train_df.iterrows():
        src = os.path.join(original_dir, row['filename'])
        dst = os.path.join(train_dir, row['filename'])
        if os.path.exists(src):
            shutil.move(src, dst)
            moved_train += 1
    
    for _, row in val_df.iterrows():
        src = os.path.join(original_dir, row['filename'])
        dst = os.path.join(val_dir, row['filename'])
        if os.path.exists(src):
            shutil.move(src, dst)
            moved_val += 1
    
    print(f"\nâœ… ç§»åŠ¨å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {moved_train} å¼ ")
    print(f"   éªŒè¯é›†: {moved_val} å¼ ")
    
    # ä¿å­˜åˆ’åˆ†ä¿¡æ¯
    split_info = {
        "total_samples": len(df),
        "train_samples": len(train_df),
        "val_samples": len(val_df),
        "num_classes": len(train_cats),
        "val_ratio": val_ratio
    }
    
    info_path = os.path.join(os.path.dirname(csv_path), "split_info.json")
    with open(info_path, "w") as f:
        json.dump(split_info, f, indent=2)
    print(f"\nâœ… åˆ’åˆ†ä¿¡æ¯å·²ä¿å­˜: {info_path}")
    
    print(f"\n{'='*60}")
    print("ğŸ‰ æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼")
    print(f"{'='*60}")
    
    return train_df, val_df


if __name__ == "__main__":
    # åŠ è½½é…ç½®
    config_path = "model/config.json"
    with open(config_path, "r") as f:
        config = json.load(f)
    
    # è§£æè·¯å¾„
    project_root = os.path.dirname(os.path.abspath(__file__))
    
    def resolve_path(path):
        if os.path.isabs(path):
            return path
        if path.startswith("../"):
            path = path[3:]
        return os.path.join(project_root, path)
    
    config["original_train_dir"] = resolve_path(config.get("original_train_dir", "submission/datasets/original_train"))
    config["train_dir"] = resolve_path(config["train_dir"])
    config["val_dir"] = resolve_path(config["val_dir"])
    config["train_label_csv"] = resolve_path(config["train_label_csv"])
    
    print("è·¯å¾„é…ç½®:")
    print(f"  åŸå§‹ç›®å½•: {config['original_train_dir']}")
    print(f"  è®­ç»ƒé›†: {config['train_dir']}")
    print(f"  éªŒè¯é›†: {config['val_dir']}")
    print(f"  CSV: {config['train_label_csv']}")
    
    # æ£€æŸ¥æ˜¯å¦å·²åˆ’åˆ†
    if os.path.exists(config["train_dir"]) and len(os.listdir(config["train_dir"])) > 0:
        print("\nâš ï¸ è®­ç»ƒé›†ç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ’åˆ†")
        print(f"   è®­ç»ƒé›†: {len(os.listdir(config['train_dir']))} å¼ ")
        print(f"   éªŒè¯é›†: {len(os.listdir(config['val_dir']))} å¼ ")
    else:
        dataset_classified(config)